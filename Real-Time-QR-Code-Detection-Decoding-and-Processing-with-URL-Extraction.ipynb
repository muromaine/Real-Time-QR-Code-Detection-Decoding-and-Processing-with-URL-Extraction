{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74c87d1",
   "metadata": {},
   "source": [
    "## Real-Time QR Code Detection, Decoding, and Processing with URL Extraction\n",
    "\n",
    "\n",
    "    Name: Rostata, Carl Louise                Student No: 202130891\n",
    "          Malveda, Jermaine Pau                           201980334\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3f7d9",
   "metadata": {},
   "source": [
    "#### Installing necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba48d70",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Obtaining dependency information for opencv-python-headless from https://files.pythonhosted.org/packages/20/44/458a0a135866f5e08266566b32ad9a182a7a059a894effe6c41a9c841ff1/opencv_python_headless-4.9.0.80-cp37-abi3-win_amd64.whl.metadata\n",
      "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pyzbar in c:\\users\\carll\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\carll\\anaconda3\\lib\\site-packages (from opencv-python-headless) (1.24.3)\n",
      "Using cached opencv_python_headless-4.9.0.80-cp37-abi3-win_amd64.whl (38.5 MB)\n",
      "Installing collected packages: opencv-python-headless\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\carll\\\\anaconda3\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless pyzbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67717d",
   "metadata": {},
   "source": [
    " - for this project, OpenCV and pyzbar already has all the tools and modules that we need for QR code detection and decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb222863",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package cv2:\n",
      "\n",
      "NAME\n",
      "    cv2 - OpenCV Python binary extension loader\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    config\n",
      "    config-3\n",
      "    cv2\n",
      "    data (package)\n",
      "    gapi (package)\n",
      "    load_config_py2\n",
      "    load_config_py3\n",
      "    mat_wrapper (package)\n",
      "    misc (package)\n",
      "    typing (package)\n",
      "    utils (package)\n",
      "    version\n",
      "\n",
      "SUBMODULES\n",
      "    Error\n",
      "    aruco\n",
      "    barcode\n",
      "    cuda\n",
      "    detail\n",
      "    dnn\n",
      "    fisheye\n",
      "    flann\n",
      "    ipp\n",
      "    ml\n",
      "    ocl\n",
      "    ogl\n",
      "    parallel\n",
      "    samples\n",
      "    segmentation\n",
      "    videoio_registry\n",
      "\n",
      "DATA\n",
      "    __all__ = []\n",
      "\n",
      "VERSION\n",
      "    4.9.0\n",
      "\n",
      "FILE\n",
      "    c:\\users\\carll\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e7e073",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function decode in module pyzbar.pyzbar:\n",
      "\n",
      "decode(image, symbols=None)\n",
      "    Decodes datamatrix barcodes in `image`.\n",
      "    \n",
      "    Args:\n",
      "        image: `numpy.ndarray`, `PIL.Image` or tuple (pixels, width, height)\n",
      "        symbols: iter(ZBarSymbol) the symbol types to decode; if `None`, uses\n",
      "            `zbar`'s default behaviour, which is to decode all symbol types.\n",
      "    \n",
      "    Returns:\n",
      "        :obj:`list` of :obj:`Decoded`: The values decoded from barcodes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e25126",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module webbrowser:\n",
      "\n",
      "NAME\n",
      "    webbrowser - Interfaces for launching and remotely controlling web browsers.\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.11/library/webbrowser.html\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        Error\n",
      "    \n",
      "    class Error(builtins.Exception)\n",
      "     |  Method resolution order:\n",
      "     |      Error\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  add_note(...)\n",
      "     |      Exception.add_note(note) --\n",
      "     |      add a note to the exception\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    get(using=None)\n",
      "        Return a browser launcher instance appropriate for the environment.\n",
      "    \n",
      "    open(url, new=0, autoraise=True)\n",
      "        Display url using the default browser.\n",
      "        \n",
      "        If possible, open url in a location determined by new.\n",
      "        - 0: the same browser window (the default).\n",
      "        - 1: a new browser window.\n",
      "        - 2: a new browser page (\"tab\").\n",
      "        If possible, autoraise raises the window (the default) or not.\n",
      "    \n",
      "    open_new(url)\n",
      "        Open url in a new window of the default browser.\n",
      "        \n",
      "        If not possible, then open url in the only browser window.\n",
      "    \n",
      "    open_new_tab(url)\n",
      "        Open url in a new page (\"tab\") of the default browser.\n",
      "        \n",
      "        If not possible, then the behavior becomes equivalent to open_new().\n",
      "    \n",
      "    register(name, klass, instance=None, *, preferred=False)\n",
      "        Register a browser connector.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Error', 'open', 'open_new', 'open_new_tab', 'get', 'regist...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\carll\\anaconda3\\lib\\webbrowser.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(webbrowser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c9dd19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function decode in module pyzbar.pyzbar:\n",
      "\n",
      "decode(image, symbols=None)\n",
      "    Decodes datamatrix barcodes in `image`.\n",
      "    \n",
      "    Args:\n",
      "        image: `numpy.ndarray`, `PIL.Image` or tuple (pixels, width, height)\n",
      "        symbols: iter(ZBarSymbol) the symbol types to decode; if `None`, uses\n",
      "            `zbar`'s default behaviour, which is to decode all symbol types.\n",
      "    \n",
      "    Returns:\n",
      "        :obj:`list` of :obj:`Decoded`: The values decoded from barcodes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83c7a8",
   "metadata": {},
   "source": [
    "####  Importing Libraries and Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3354c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pyzbar.pyzbar import decode\n",
    "from pyzbar.pyzbar import ZBarSymbol\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5d60c",
   "metadata": {},
   "source": [
    "- cv2: used for our image processing \n",
    "- numpy: used for numerical computation\n",
    "- decode from pzybar -  function used to decode the QR codes from the frames (images in video)\n",
    "-  webbrowser - module used to open URLs in web browser when the algorithm detected a URL in the QR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e95be9",
   "metadata": {},
   "source": [
    "#### Detecting the QR Code Function\n",
    "\n",
    "- We used a defined function so that it can be easily debugged and repeatedly used in our main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa77cf92",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function rectangle:\n",
      "\n",
      "rectangle(...)\n",
      "    rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
      "    .   @brief Draws a simple, thick, or filled up-right rectangle.\n",
      "    .   \n",
      "    .   The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners\n",
      "    .   are pt1 and pt2.\n",
      "    .   \n",
      "    .   @param img Image.\n",
      "    .   @param pt1 Vertex of the rectangle.\n",
      "    .   @param pt2 Vertex of the rectangle opposite to pt1 .\n",
      "    .   @param color Rectangle color or brightness (grayscale image).\n",
      "    .   @param thickness Thickness of lines that make up the rectangle. Negative values, like #FILLED,\n",
      "    .   mean that the function has to draw a filled rectangle.\n",
      "    .   @param lineType Type of the line. See #LineTypes\n",
      "    .   @param shift Number of fractional bits in the point coordinates.\n",
      "    \n",
      "    \n",
      "    \n",
      "    rectangle(img, rec, color[, thickness[, lineType[, shift]]]) -> img\n",
      "    .   @overload\n",
      "    .   \n",
      "    .   use `rec` parameter as alternative specification of the drawn rectangle: `r.tl() and\n",
      "    .   r.br()-Point(1,1)` are opposite corners\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.rectangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa44b87d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function putText:\n",
      "\n",
      "putText(...)\n",
      "    putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]]) -> img\n",
      "    .   @brief Draws a text string.\n",
      "    .   \n",
      "    .   The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered\n",
      "    .   using the specified font are replaced by question marks. See #getTextSize for a text rendering code\n",
      "    .   example.\n",
      "    .   \n",
      "    .   @param img Image.\n",
      "    .   @param text Text string to be drawn.\n",
      "    .   @param org Bottom-left corner of the text string in the image.\n",
      "    .   @param fontFace Font type, see #HersheyFonts.\n",
      "    .   @param fontScale Font scale factor that is multiplied by the font-specific base size.\n",
      "    .   @param color Text color.\n",
      "    .   @param thickness Thickness of the lines used to draw a text.\n",
      "    .   @param lineType Line type. See #LineTypes\n",
      "    .   @param bottomLeftOrigin When true, the image data origin is at the bottom-left corner. Otherwise,\n",
      "    .   it is at the top-left corner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.putText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e99c23f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function polylines:\n",
      "\n",
      "polylines(...)\n",
      "    polylines(img, pts, isClosed, color[, thickness[, lineType[, shift]]]) -> img\n",
      "    .   @brief Draws several polygonal curves.\n",
      "    .   \n",
      "    .   @param img Image.\n",
      "    .   @param pts Array of polygonal curves.\n",
      "    .   @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,\n",
      "    .   the function draws a line from the last vertex of each curve to its first vertex.\n",
      "    .   @param color Polyline color.\n",
      "    .   @param thickness Thickness of the polyline edges.\n",
      "    .   @param lineType Type of the line segments. See #LineTypes\n",
      "    .   @param shift Number of fractional bits in the vertex coordinates.\n",
      "    .   \n",
      "    .   The function cv::polylines draws one or more polygonal curves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.polylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e6989c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function GaussianBlur:\n",
      "\n",
      "GaussianBlur(...)\n",
      "    GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]]) -> dst\n",
      "    .   @brief Blurs an image using a Gaussian filter.\n",
      "    .   \n",
      "    .   The function convolves the source image with the specified Gaussian kernel. In-place filtering is\n",
      "    .   supported.\n",
      "    .   \n",
      "    .   @param src input image; the image can have any number of channels, which are processed\n",
      "    .   independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
      "    .   @param dst output image of the same size and type as src.\n",
      "    .   @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be\n",
      "    .   positive and odd. Or, they can be zero's and then they are computed from sigma.\n",
      "    .   @param sigmaX Gaussian kernel standard deviation in X direction.\n",
      "    .   @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be\n",
      "    .   equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,\n",
      "    .   respectively (see #getGaussianKernel for details); to fully control the result regardless of\n",
      "    .   possible future modifications of all this semantics, it is recommended to specify all of ksize,\n",
      "    .   sigmaX, and sigmaY.\n",
      "    .   @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.\n",
      "    .   \n",
      "    .   @sa  sepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.GaussianBlur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "448b8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Convert frame to grayscale\n",
    "    # We used cv2.COLOR to perform a conversion of the frame from BRG to gray (same with colors' rgb2gray function)\n",
    "    # This is to simplify the QR code image/frame to reduce processing task\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur for noise reduction\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9b1e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_QRcode(frame):\n",
    "    processed_frame = preprocess_frame(frame)\n",
    "    \n",
    "    # Detect QR codes\n",
    "    QRcodes = decode(processed_frame, symbols=[ZBarSymbol.QRCODE])\n",
    "    \"\"\"\"\n",
    "    The decode function takes the frane as a paramerter and decodes the QR Code \n",
    "    \"\"\"\n",
    "    for QRcode in QRcodes: \n",
    "        \"\"\"\"\n",
    "        Since we are dealing with frames, we need to iterate over the detected QR codes\n",
    "        \"\"\"\n",
    "        (x, y, w, h) = QRcode.rect \n",
    "        #We extract the coordinate points of the detected QR\n",
    "        points = np.array([QRcode.polygon], np.int32)\n",
    "        cv2.polylines(frame, (points), True, (0, 255, 0), 3)\n",
    "        #This draws a polygon around the dimension of the QR code\n",
    "        QRcodeData = QRcode.data.decode(\"utf-8\")\n",
    "        #this decodes the detected QR code and encodes the data using UTF-8 \n",
    "        QRcodeType = QRcode.type\n",
    "        # this identifies the detected QR code type for displaying\n",
    "        text = \"{} ({})\".format(QRcodeData, QRcodeType) #encapsulates the QR code data and type into a string variable for displaying\n",
    "        cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        #displays the formatted text into the frame (Above the rectangle in the QR code)\n",
    "        print(\"[INFO] Found {} QR Code: {}\".format(QRcodeType, QRcodeData))\n",
    "        return QRcodeData\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a4c7c",
   "metadata": {},
   "source": [
    "#### Initializing Video Capture Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f7bf50f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VideoCapture in module cv2:\n",
      "\n",
      "class VideoCapture(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  get(...)\n",
      " |      get(propId) -> retval\n",
      " |      .   @brief Returns the specified VideoCapture property\n",
      " |      .   \n",
      " |      .       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n",
      " |      .       or one from @ref videoio_flags_others\n",
      " |      .       @return Value for the specified property. Value 0 is returned when querying a property that is\n",
      " |      .       not supported by the backend used by the VideoCapture instance.\n",
      " |      .   \n",
      " |      .       @note Reading / writing properties involves many layers. Some unexpected result might happens\n",
      " |      .       along this chain.\n",
      " |      .       @code{.txt}\n",
      " |      .       VideoCapture -> API Backend -> Operating System -> Device Driver -> Device Hardware\n",
      " |      .       @endcode\n",
      " |      .       The returned value might be different from what really used by the device or it could be encoded\n",
      " |      .       using device dependent rules (eg. steps or percentage). Effective behaviour depends from device\n",
      " |      .       driver and API Backend\n",
      " |  \n",
      " |  getBackendName(...)\n",
      " |      getBackendName() -> retval\n",
      " |      .   @brief Returns used backend API name\n",
      " |      .   \n",
      " |      .        @note Stream should be opened.\n",
      " |  \n",
      " |  getExceptionMode(...)\n",
      " |      getExceptionMode() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  grab(...)\n",
      " |      grab() -> retval\n",
      " |      .   @brief Grabs the next frame from video file or capturing device.\n",
      " |      .   \n",
      " |      .       @return `true` (non-zero) in the case of success.\n",
      " |      .   \n",
      " |      .       The method/function grabs the next frame from video file or camera and returns true (non-zero) in\n",
      " |      .       the case of success.\n",
      " |      .   \n",
      " |      .       The primary use of the function is in multi-camera environments, especially when the cameras do not\n",
      " |      .       have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that\n",
      " |      .       call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way\n",
      " |      .       the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames\n",
      " |      .       from different cameras will be closer in time.\n",
      " |      .   \n",
      " |      .       Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the\n",
      " |      .       correct way of retrieving data from it is to call VideoCapture::grab() first and then call\n",
      " |      .       VideoCapture::retrieve() one or more times with different values of the channel parameter.\n",
      " |      .   \n",
      " |      .       @ref tutorial_kinect_openni\n",
      " |  \n",
      " |  isOpened(...)\n",
      " |      isOpened() -> retval\n",
      " |      .   @brief Returns true if video capturing has been initialized already.\n",
      " |      .   \n",
      " |      .       If the previous call to VideoCapture constructor or VideoCapture::open() succeeded, the method returns\n",
      " |      .       true.\n",
      " |  \n",
      " |  open(...)\n",
      " |      open(filename[, apiPreference]) -> retval\n",
      " |      .   @brief  Opens a video file or a capturing device or an IP video stream for video capturing.\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       Parameters are same as the constructor VideoCapture(const String& filename, int apiPreference = CAP_ANY)\n",
      " |      .       @return `true` if the file has been successfully opened\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      open(filename, apiPreference, params) -> retval\n",
      " |      .   @brief  Opens a video file or a capturing device or an IP video stream for video capturing with API Preference and parameters\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       The `params` parameter allows to specify extra parameters encoded as pairs `(paramId_1, paramValue_1, paramId_2, paramValue_2, ...)`.\n",
      " |      .       See cv::VideoCaptureProperties\n",
      " |      .   \n",
      " |      .       @return `true` if the file has been successfully opened\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      open(index[, apiPreference]) -> retval\n",
      " |      .   @brief  Opens a camera for video capturing\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       Parameters are same as the constructor VideoCapture(int index, int apiPreference = CAP_ANY)\n",
      " |      .       @return `true` if the camera has been successfully opened.\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      open(index, apiPreference, params) -> retval\n",
      " |      .   @brief  Opens a camera for video capturing with API Preference and parameters\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       The `params` parameter allows to specify extra parameters encoded as pairs `(paramId_1, paramValue_1, paramId_2, paramValue_2, ...)`.\n",
      " |      .       See cv::VideoCaptureProperties\n",
      " |      .   \n",
      " |      .       @return `true` if the camera has been successfully opened.\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |  \n",
      " |  read(...)\n",
      " |      read([, image]) -> retval, image\n",
      " |      .   @brief Grabs, decodes and returns the next video frame.\n",
      " |      .   \n",
      " |      .       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n",
      " |      .       @return `false` if no frames has been grabbed\n",
      " |      .   \n",
      " |      .       The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the\n",
      " |      .       most convenient method for reading video files or capturing data from decode and returns the just\n",
      " |      .       grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more\n",
      " |      .       frames in video file), the method returns false and the function returns empty image (with %cv::Mat, test it with Mat::empty()).\n",
      " |      .   \n",
      " |      .       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n",
      " |      .       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n",
      " |      .       cvCloneImage and then do whatever you want with the copy.\n",
      " |  \n",
      " |  release(...)\n",
      " |      release() -> None\n",
      " |      .   @brief Closes video file or capturing device.\n",
      " |      .   \n",
      " |      .       The method is automatically called by subsequent VideoCapture::open and by VideoCapture\n",
      " |      .       destructor.\n",
      " |      .   \n",
      " |      .       The C function also deallocates memory and clears \\*capture pointer.\n",
      " |  \n",
      " |  retrieve(...)\n",
      " |      retrieve([, image[, flag]]) -> retval, image\n",
      " |      .   @brief Decodes and returns the grabbed video frame.\n",
      " |      .   \n",
      " |      .       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n",
      " |      .       @param flag it could be a frame index or a driver specific flag\n",
      " |      .       @return `false` if no frames has been grabbed\n",
      " |      .   \n",
      " |      .       The method decodes and returns the just grabbed frame. If no frames has been grabbed\n",
      " |      .       (camera has been disconnected, or there are no more frames in video file), the method returns false\n",
      " |      .       and the function returns an empty image (with %cv::Mat, test it with Mat::empty()).\n",
      " |      .   \n",
      " |      .       @sa read()\n",
      " |      .   \n",
      " |      .       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n",
      " |      .       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n",
      " |      .       cvCloneImage and then do whatever you want with the copy.\n",
      " |  \n",
      " |  set(...)\n",
      " |      set(propId, value) -> retval\n",
      " |      .   @brief Sets a property in the VideoCapture.\n",
      " |      .   \n",
      " |      .       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n",
      " |      .       or one from @ref videoio_flags_others\n",
      " |      .       @param value Value of the property.\n",
      " |      .       @return `true` if the property is supported by backend used by the VideoCapture instance.\n",
      " |      .       @note Even if it returns `true` this doesn't ensure that the property\n",
      " |      .       value has been accepted by the capture device. See note in VideoCapture::get()\n",
      " |  \n",
      " |  setExceptionMode(...)\n",
      " |      setExceptionMode(enable) -> None\n",
      " |      .   Switches exceptions mode\n",
      " |      .        *\n",
      " |      .        * methods raise exceptions if not successful instead of returning an error code\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  waitAny(...)\n",
      " |      waitAny(streams[, timeoutNs]) -> retval, readyIndex\n",
      " |      .   @brief Wait for ready frames from VideoCapture.\n",
      " |      .   \n",
      " |      .       @param streams input video streams\n",
      " |      .       @param readyIndex stream indexes with grabbed frames (ready to use .retrieve() to fetch actual frame)\n",
      " |      .       @param timeoutNs number of nanoseconds (0 - infinite)\n",
      " |      .       @return `true` if streamReady is not empty\n",
      " |      .   \n",
      " |      .       @throws Exception %Exception on stream errors (check .isOpened() to filter out malformed streams) or VideoCapture type is not supported\n",
      " |      .   \n",
      " |      .       The primary use of the function is in multi-camera environments.\n",
      " |      .       The method fills the ready state vector, grabs video frame, if camera is ready.\n",
      " |      .   \n",
      " |      .       After this call use VideoCapture::retrieve() to decode and fetch frame data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (cv2.VideoCapture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d82c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # Using 0 to use webcam as the source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd1f1a",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42ac0819",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function cvtColor:\n",
      "\n",
      "cvtColor(...)\n",
      "    cvtColor(src, code[, dst[, dstCn]]) -> dst\n",
      "    .   @brief Converts an image from one color space to another.\n",
      "    .   \n",
      "    .   The function converts an input image from one color space to another. In case of a transformation\n",
      "    .   to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n",
      "    .   that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n",
      "    .   bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n",
      "    .   component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n",
      "    .   sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n",
      "    .   \n",
      "    .   The conventional ranges for R, G, and B channel values are:\n",
      "    .   -   0 to 255 for CV_8U images\n",
      "    .   -   0 to 65535 for CV_16U images\n",
      "    .   -   0 to 1 for CV_32F images\n",
      "    .   \n",
      "    .   In case of linear transformations, the range does not matter. But in case of a non-linear\n",
      "    .   transformation, an input RGB image should be normalized to the proper value range to get the correct\n",
      "    .   results, for example, for RGB \\f$\\rightarrow\\f$ L\\*u\\*v\\* transformation. For example, if you have a\n",
      "    .   32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n",
      "    .   have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,\n",
      "    .   you need first to scale the image down:\n",
      "    .   @code\n",
      "    .       img *= 1./255;\n",
      "    .       cvtColor(img, img, COLOR_BGR2Luv);\n",
      "    .   @endcode\n",
      "    .   If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many\n",
      "    .   applications, this will not be noticeable but it is recommended to use 32-bit images in applications\n",
      "    .   that need the full range of colors or that convert an image before an operation and then convert\n",
      "    .   back.\n",
      "    .   \n",
      "    .   If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n",
      "    .   range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n",
      "    .   \n",
      "    .   @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n",
      "    .   floating-point.\n",
      "    .   @param dst output image of the same size and depth as src.\n",
      "    .   @param code color space conversion code (see #ColorConversionCodes).\n",
      "    .   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n",
      "    .   channels is derived automatically from src and code.\n",
      "    .   \n",
      "    .   @see @ref imgproc_color_conversions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.cvtColor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abf661c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found QRCODE QR Code: https://dlsud.edu20.org/\n",
      "Do you want to open this link? (y/n): n\n",
      "[INFO] Found QRCODE QR Code: https://dlsud.edu20.org/\n",
      "Do you want to open this link? (y/n): n\n",
      "[INFO] Found QRCODE QR Code: https://dlsud.edu20.org/\n",
      "Do you want to open this link? (y/n): y\n",
      "[INFO] Found QRCODE QR Code: https://dlsud.edu20.org/\n",
      "Do you want to open this link? (y/n): y\n"
     ]
    }
   ],
   "source": [
    "scan_image = cv2.imread('scan.png', cv2.IMREAD_UNCHANGED)\n",
    "# for the watermark, we used the imread function of cv2\n",
    "# we utilized cv2.IMREAD_UNCHANGED to upload the image as it is so that it can\n",
    "# retain its transparency on certain parts of the image.\n",
    "\n",
    "# We used an infinite loop to continuously capture frames from the webcam source (video)\n",
    "while True:\n",
    "    captured, frame = cap.read()\n",
    "    if not captured:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "    # We used a variable \"captured\" as a boolean identifier if the current frame\n",
    "    # was captured. We utilized a conditional statement if-then to break the loop\n",
    "    # or stop the code if the program failed to capture the frames.\n",
    "    \n",
    "    #using preprocessing of the frame\n",
    "    processed_frame = preprocess_frame(frame)\n",
    "    \n",
    "    # By using the decode function from pyzbar, we decoded the QR code content present in the pre-processed frame\n",
    "    Qrcode = decode(processed_frame, symbols=[ZBarSymbol.QRCODE])\n",
    "\n",
    "    # Watermark using Alpha Blending technique\n",
    "    scan_height, scan_width, _ = scan_image.shape\n",
    "        # we retrieved the image dimensions using the shape attribute and stored them inside separate variables\n",
    "    start_x = int((frame.shape[1] - scan_width) / 2)\n",
    "    start_y = int((frame.shape[0] - scan_height) / 2)\n",
    "        # This lines computes the starting coordinates of the mask to ensure that it is centered within the set webcam frame\n",
    "    mask = scan_image[:, :, 3] / 255.0\n",
    "        # Here we are extracting the alpha channel of the scan_image to determine the transparency of each pixel.\n",
    "    for c in range(3):\n",
    "        frame[start_y:start_y + scan_height, start_x:start_x + scan_width, c] = \\\n",
    "            scan_image[:, :, c] * mask + frame[start_y:start_y + scan_height, start_x:start_x + scan_width, c] * (\n",
    "                    1 - mask)\n",
    "        # This code applies the mask onto the frame using alpha blending. \n",
    "        # This iterates over each color channel RGB of the frame\n",
    "        # It basically calculates the blended pixel value based on the alpha mask and overlays the\n",
    "        # watermak onto the frame.\n",
    "        \n",
    "    # We will now use the detect_QRcode function we defined earlier.\n",
    "    # We first stored the return value of detect_QRcode function (none/QR code decoded data)\n",
    "    # We used a conditional statement to check if there is a returned data, if there is a returned value,\n",
    "    # then the code will proceed to the displaying of the QR Code data, if none, then nothing will happen.\n",
    "    # the frame will show the webcam feed as usual.\n",
    "    QRcode_data = detect_QRcode(frame)\n",
    "    if QRcode_data:\n",
    "        cv2.putText(frame, \"QR Code: {}\".format(QRcode_data), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            #this displays the QR Code decoded information in the upper most area of the frame (information in the white bar)\n",
    "            \n",
    "        #Extraction of URL and redirecting\n",
    "            # Check if decoded data isa URL, if it is, then a prompt will show up in the output log\n",
    "        if QRcode_data.startswith(\"http://\") or QRcode_data.startswith(\"https://\"):\n",
    "            response = input(\"Do you want to open this link? (y/n): \")\n",
    "            # if the response is 'y', then the webbrowser.open() function will redirect the QR Code link\n",
    "            # to a tab in the browser. If 'n' then nothing happens, and the code proceeds to loop.\n",
    "            if response.lower() == 'y':\n",
    "                webbrowser.open(QRcode_data)\n",
    "                \n",
    "    # Frame title\n",
    "    cv2.imshow(\"QR Code Detector\", frame)\n",
    "    \n",
    "    # Using CV2.waitkey() function, we are waiting for a user input of 'x' from the keyboard\n",
    "    # If the user pressed x, then the code breaks out of the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('x'):\n",
    "        break\n",
    "\n",
    "# When the code breaks out of the loop, the cap.release() releases the video capture \n",
    "# and cv2.destroyAllWindows() closes the window of the frame currently open.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
